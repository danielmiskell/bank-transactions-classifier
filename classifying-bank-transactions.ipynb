{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Bank Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview:\n",
    "\n",
    "My objective with this notebook is to demonstrate my solution to the data science challenge given, to input \n",
    "bank transactional data and return a data structure summarizing a user's regularly recurring transactions as output.\\\n",
    "\\\n",
    "My solution is a rule based proceedure, it parses information from a bank transaction description to group similarly named transactions together. Before a series of functions which filter out groups unlikely to be recurring. I used a rule based model here because its easily interpretable, explainable and our assumptions known. As well as being fast. However, we shall see that, with the complexity of the problem it makes patching up special cases difficult. As well as not providing more descriptive information which could be useful, that a more complex model could provide. \\\n",
    "\\\n",
    "I provide my first model solution, before reviewing, then making improvements before the final model. I review my final model, before concluding and discussing my next steps - if they were to be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Loading, Sorting and Previewing](#loading)\n",
    "2.    [First Model](#firstmodel)\n",
    "3.    [First Model Review](#firstmodelnreview)\n",
    "4.    [Final Model](#finalmodel)\n",
    "5.    [Final Model Review](#finalmodelreview)\n",
    "6.    [Conclusion & Next Steps](#nextsteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading, Sorting and Previewing.<a name=\"loading\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data=\"example_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(path_to_data):\n",
    "    \"\"\"Returns sorted transactions, in ascending order by 'made_on' date after loading data.\"\"\"\n",
    "    transactions = pd.read_csv(path_to_data, index_col=0)\n",
    "    transactions['made_on'] = pd.to_datetime(transactions.made_on)\n",
    "    transactions = transactions.sort_values('made_on')\n",
    "    return transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = preprocessing(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>made_on</th>\n",
       "      <th>original_description</th>\n",
       "      <th>amount_cents</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2018-07-22</td>\n",
       "      <td>HAWKS CLUB ON 20 JUL CLP</td>\n",
       "      <td>3000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2018-07-22</td>\n",
       "      <td>SAINSBURYS S/MKTS ON 21 JUL CLP</td>\n",
       "      <td>505</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2018-07-23</td>\n",
       "      <td>WASABI CAMBRIDGE ON 22 JUL CLP</td>\n",
       "      <td>1710</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2018-07-23</td>\n",
       "      <td>NYA*Cambridge Vend ON 22 JUL BCC</td>\n",
       "      <td>250</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2018-07-23</td>\n",
       "      <td>AROMI CAFFE LTD ON 22 JUL CLP</td>\n",
       "      <td>410</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  user_id    made_on  \\\n",
       "617  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2018-07-22   \n",
       "891  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2018-07-22   \n",
       "940  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2018-07-23   \n",
       "616  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2018-07-23   \n",
       "941  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2018-07-23   \n",
       "\n",
       "                 original_description  amount_cents  income  \n",
       "617          HAWKS CLUB ON 20 JUL CLP          3000   False  \n",
       "891   SAINSBURYS S/MKTS ON 21 JUL CLP           505   False  \n",
       "940    WASABI CAMBRIDGE ON 22 JUL CLP          1710   False  \n",
       "616  NYA*Cambridge Vend ON 22 JUL BCC           250   False  \n",
       "941     AROMI CAFFE LTD ON 22 JUL CLP           410   False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1210 entries, 617 to 1208\n",
      "Data columns (total 5 columns):\n",
      "user_id                 1210 non-null object\n",
      "made_on                 1210 non-null datetime64[ns]\n",
      "original_description    1210 non-null object\n",
      "amount_cents            1210 non-null int64\n",
      "income                  1210 non-null bool\n",
      "dtypes: bool(1), datetime64[ns](1), int64(1), object(2)\n",
      "memory usage: 48.4+ KB\n"
     ]
    }
   ],
   "source": [
    "transactions.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "No missing values to deal with, thats good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model.<a name=\"firstmodek\"></a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here I lay out my first models functionality to process the data from input to output. I need a way to group transactions into those which are recurring. I can see from viewing the first 5 lines that 'original_description' contains a lot of different infomation. The business entity making the transaction, the date, or the location.\n",
    "\n",
    "I shall try to group the transactions by the first word of 'original_decription', assuming (potentially naively) that recurring payments have identical first words in 'original_description'. \n",
    "\n",
    "I shall discuss potential improvements to this assumption later. We could also parse the date given in 'original_description' which could give us valuable information, such as a more accurate date for when transactions were made, but for now I will ignore this information. We could also parse the second or third word which may hold useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_original_description(transations):\n",
    "    \"\"\"Returns transactions after adding new column equal to first word of 'original_description'.\n",
    "    Splits 'original_description' by \" \" and \"*\" characters.\n",
    "    \"\"\"\n",
    "    original_description_split_astrix_space = transactions['original_description'].str.split(\" |\\*\", n=3, expand=True)\n",
    "    transactions['original_description_first_word'] = original_description_split_astrix_space[0]\n",
    "    \n",
    "    return transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>made_on</th>\n",
       "      <th>original_description</th>\n",
       "      <th>amount_cents</th>\n",
       "      <th>income</th>\n",
       "      <th>original_description_first_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2018-07-22</td>\n",
       "      <td>HAWKS CLUB ON 20 JUL CLP</td>\n",
       "      <td>3000</td>\n",
       "      <td>False</td>\n",
       "      <td>HAWKS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2018-07-22</td>\n",
       "      <td>SAINSBURYS S/MKTS ON 21 JUL CLP</td>\n",
       "      <td>505</td>\n",
       "      <td>False</td>\n",
       "      <td>SAINSBURYS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2018-07-23</td>\n",
       "      <td>WASABI CAMBRIDGE ON 22 JUL CLP</td>\n",
       "      <td>1710</td>\n",
       "      <td>False</td>\n",
       "      <td>WASABI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2018-07-23</td>\n",
       "      <td>NYA*Cambridge Vend ON 22 JUL BCC</td>\n",
       "      <td>250</td>\n",
       "      <td>False</td>\n",
       "      <td>NYA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2018-07-23</td>\n",
       "      <td>AROMI CAFFE LTD ON 22 JUL CLP</td>\n",
       "      <td>410</td>\n",
       "      <td>False</td>\n",
       "      <td>AROMI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  user_id    made_on  \\\n",
       "617  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2018-07-22   \n",
       "891  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2018-07-22   \n",
       "940  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2018-07-23   \n",
       "616  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2018-07-23   \n",
       "941  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2018-07-23   \n",
       "\n",
       "                 original_description  amount_cents  income  \\\n",
       "617          HAWKS CLUB ON 20 JUL CLP          3000   False   \n",
       "891   SAINSBURYS S/MKTS ON 21 JUL CLP           505   False   \n",
       "940    WASABI CAMBRIDGE ON 22 JUL CLP          1710   False   \n",
       "616  NYA*Cambridge Vend ON 22 JUL BCC           250   False   \n",
       "941     AROMI CAFFE LTD ON 22 JUL CLP           410   False   \n",
       "\n",
       "    original_description_first_word  \n",
       "617                           HAWKS  \n",
       "891                      SAINSBURYS  \n",
       "940                          WASABI  \n",
       "616                             NYA  \n",
       "941                           AROMI  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions = parse_original_description(transactions)\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I shall make the assumption recurring assumptions three or more transactions, only considering groups of transactions with three or more transactions by removing groups which do not satisfy this condition.\n",
    "The below code deletes groups with a number of transactions below lower_minimum, default value of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_groups_too_small(transactions, info, lower_minimum=3):\n",
    "    \"\"\"Returns transactions after removing groups with less than three transactions.\n",
    "    \n",
    "    input info: whether or not to display processing information,\n",
    "    input lower_minimum: lower limit on the size of a group to not be removed, default = 3.\n",
    "    \"\"\"\n",
    "    # save transactions and its respective groups to track changes.\n",
    "    input_transactions = transactions\n",
    "    transactions_grouped = transactions.groupby('original_description_first_word')\n",
    "    input_transactions_grouped = transactions_grouped\n",
    "    \n",
    "    # if group has less than the lower_minimum number transactions remove group from transactions dataframe.\n",
    "    for name, group in transactions_grouped:\n",
    "        if group['user_id'].count() < lower_minimum:\n",
    "            transactions = transactions.drop(transactions_grouped.get_group(name).index)\n",
    "\n",
    "    transactions_grouped = transactions.groupby('original_description_first_word')\n",
    "    \n",
    "    # print information\n",
    "    if info:\n",
    "        print(\"Old number of transactions: {} \\n\\\n",
    "        Old number of groups: {} \\n\\\n",
    "        Remaining number of transactions: {} \\n\\\n",
    "        Remaining number of groups: {} \\n\"\\\n",
    "        .format(input_transactions['user_id'].count(), len(input_transactions_grouped), transactions['user_id'].count(), len(transactions_grouped)))\n",
    "    \n",
    "    return transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old number of transactions: 1210 \n",
      "        Old number of groups: 291 \n",
      "        Remaining number of transactions: 928 \n",
      "        Remaining number of groups: 61 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions = remove_groups_too_small(transactions, info=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that I've grouped my transactions and removed groups too small, I want see the time differences between transactions in a group. This will give me vital information as to whether they're recurring or not.\n",
    "\n",
    "I will create a column a new column called \"time_diff_forward\". It's value for each transaction will be the time in days till the next transaction in its corresponding group.\n",
    "\n",
    "For the most recent transaction, this entry will be NaN, as there is no more transactions to compare it to. This will allow me to ignore this value when calculating the mean for the time difference forward - on a specific group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_create_time_difference_column(group):\n",
    "    \"\"\"Returns a group after adding 'time_difference_forward' which is the time difference in days to \n",
    "    next transaction in its group.\n",
    "    Most recent transaction declared as NaN.\n",
    "    \"\"\"\n",
    "    group = group.reset_index(drop=False)\n",
    "    group['time_diff_forward'] = np.nan\n",
    "    \n",
    "    # loops over transactions in the group, for a transaction calculates time difference (in days) between itself and the next transaction\n",
    "    for i in group.index:\n",
    "        t1 = group.loc[i, 'made_on']\n",
    "        if i+1<len(group.index):\n",
    "            t2 = group.loc[i+1, 'made_on']\n",
    "        group['time_diff_forward'].loc[i] = (t2 - t1).days\n",
    "    \n",
    "    # sets most recent transaction to NaN\n",
    "    group.at[group.index[-1], 'time_diff_forward'] = np.nan\n",
    "\n",
    "    return group"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now I have this column I can calculate:\n",
    "\n",
    "\"entity_name\" - as the 'original_description_first_word'\n",
    "\n",
    "\"income\" - True if all values in group are True, otherwise False. \n",
    "I'm making the assumption that a credit transaction will only ever pay in, otherwise its a debit transaction where it pays out of your account.\n",
    "This assumption may break where you have recurring payments with a entity who also pays you. For example, you receive monthly dividends as income from a stocks & shares business, but you also pay them an annual fee to hold your capital investments.\n",
    "\n",
    "\"period_days\" - the mean value for the 'time_diff_forward' values. The time difference forward between each transaction. Rounded to 1 d.p.\n",
    "\n",
    "\"typical_amount_cents\" - the mean value over transaction values for 'amount_cents'. Rounded to one cent.\n",
    "Since we haven't filtered any of the transactions in a group which could skew our mean, for example, a monthly phone subscription of £30 but with a first time payment of £200. I will consider this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_model(transactions, info=True, example_name=None):\n",
    "    \"\"\" First model for get_recurring_transactions() function.\n",
    "    Returns recurring, a list of dictionaries where each dictionary is of form:\n",
    "    \n",
    "    {\"entity_name\": A common description of the regularly occurring transactions,\n",
    "    \"income\": A boolean whether the transaction is debit (True) or credit (False),\n",
    "    \"period_days\": The expected interval in days between consecutive transactions for that entity, \n",
    "    \"typical_amount_cents\": The typical amount in cents}\n",
    "    \n",
    "    input info: whether or not to display processing information,\n",
    "    input example_name: name of example to make it easy to select a specific group example for analysis later\n",
    "    \"\"\"  \n",
    "    transactions = parse_original_description(transactions)\n",
    "    \n",
    "    # Code to make it easy to investigate specific example e.g 'Spotify'\n",
    "    if example_name is not None:\n",
    "        transactions = transactions[transactions['original_description_first_word'] == example_name]\n",
    "                                                                                           \n",
    "    transactions = remove_groups_too_small(transactions, info)\n",
    "    \n",
    "    recurring = []\n",
    "    \n",
    "    for name, group in transactions.groupby('original_description_first_word'):\n",
    "        group = group_create_time_difference_column(group)\n",
    "        \n",
    "        if name == example_name:\n",
    "            print(\"{} transactions:\\n{}\".format(example_name, group))\n",
    "            \n",
    "        recurring.append({\"entity_name\": name, \"income\": group['income'].all() ,\\\n",
    "                          \"period_days\": round(group['time_diff_forward'].mean(skipna=True)),\\\n",
    "                          \"typical_amount_cents\": int(round(group['amount_cents'].mean()))})\n",
    "    return recurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old number of transactions: 1210 \n",
      "        Old number of groups: 291 \n",
      "        Remaining number of transactions: 928 \n",
      "        Remaining number of groups: 61 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_name': 'AIRBNB',\n",
       "  'income': False,\n",
       "  'period_days': 20.0,\n",
       "  'typical_amount_cents': 66687},\n",
       " {'entity_name': 'AMZN',\n",
       "  'income': False,\n",
       "  'period_days': 11.0,\n",
       "  'typical_amount_cents': 1913},\n",
       " {'entity_name': 'AMZNMktplace',\n",
       "  'income': False,\n",
       "  'period_days': 24.0,\n",
       "  'typical_amount_cents': 1361},\n",
       " {'entity_name': 'APSON',\n",
       "  'income': True,\n",
       "  'period_days': 6.0,\n",
       "  'typical_amount_cents': 73545},\n",
       " {'entity_name': 'AQUARIUM',\n",
       "  'income': False,\n",
       "  'period_days': 102.0,\n",
       "  'typical_amount_cents': 1767},\n",
       " {'entity_name': 'AROMI',\n",
       "  'income': False,\n",
       "  'period_days': 52.0,\n",
       "  'typical_amount_cents': 675},\n",
       " {'entity_name': 'Amazon',\n",
       "  'income': False,\n",
       "  'period_days': 40.0,\n",
       "  'typical_amount_cents': 1081},\n",
       " {'entity_name': 'Amazon.co.uk',\n",
       "  'income': False,\n",
       "  'period_days': 46.0,\n",
       "  'typical_amount_cents': 1497},\n",
       " {'entity_name': 'BILL',\n",
       "  'income': False,\n",
       "  'period_days': 28.0,\n",
       "  'typical_amount_cents': 25008},\n",
       " {'entity_name': 'BON',\n",
       "  'income': False,\n",
       "  'period_days': 4.0,\n",
       "  'typical_amount_cents': 680},\n",
       " {'entity_name': 'BONAPPETIT',\n",
       "  'income': False,\n",
       "  'period_days': 12.0,\n",
       "  'typical_amount_cents': 500},\n",
       " {'entity_name': 'BOOTS',\n",
       "  'income': False,\n",
       "  'period_days': 16.0,\n",
       "  'typical_amount_cents': 855},\n",
       " {'entity_name': 'BOOTS,CAMBRIDGE',\n",
       "  'income': False,\n",
       "  'period_days': 72.0,\n",
       "  'typical_amount_cents': 294},\n",
       " {'entity_name': 'CAMBRIDGE',\n",
       "  'income': False,\n",
       "  'period_days': 31.0,\n",
       "  'typical_amount_cents': 5436},\n",
       " {'entity_name': 'DELIVEROO.CO.UK',\n",
       "  'income': False,\n",
       "  'period_days': 104.0,\n",
       "  'typical_amount_cents': 1633},\n",
       " {'entity_name': 'DROPBOX',\n",
       "  'income': False,\n",
       "  'period_days': 31.0,\n",
       "  'typical_amount_cents': 919},\n",
       " {'entity_name': 'ENGOCHA',\n",
       "  'income': False,\n",
       "  'period_days': 32.0,\n",
       "  'typical_amount_cents': 969},\n",
       " {'entity_name': 'FIVE',\n",
       "  'income': False,\n",
       "  'period_days': 45.0,\n",
       "  'typical_amount_cents': 1605},\n",
       " {'entity_name': 'FOOTBALL',\n",
       "  'income': False,\n",
       "  'period_days': 36.0,\n",
       "  'typical_amount_cents': 7107},\n",
       " {'entity_name': 'GREATER',\n",
       "  'income': False,\n",
       "  'period_days': 31.0,\n",
       "  'typical_amount_cents': 2128},\n",
       " {'entity_name': 'GREEN',\n",
       "  'income': False,\n",
       "  'period_days': 58.0,\n",
       "  'typical_amount_cents': 1740},\n",
       " {'entity_name': 'Gaucho',\n",
       "  'income': False,\n",
       "  'period_days': 10.0,\n",
       "  'typical_amount_cents': 1374},\n",
       " {'entity_name': 'H',\n",
       "  'income': False,\n",
       "  'period_days': 76.0,\n",
       "  'typical_amount_cents': 3466},\n",
       " {'entity_name': 'HAWKS',\n",
       "  'income': False,\n",
       "  'period_days': 118.0,\n",
       "  'typical_amount_cents': 3167},\n",
       " {'entity_name': 'INVESTMENT',\n",
       "  'income': False,\n",
       "  'period_days': 34.0,\n",
       "  'typical_amount_cents': 28667},\n",
       " {'entity_name': 'ITUNES.COM/BILL',\n",
       "  'income': False,\n",
       "  'period_days': 44.0,\n",
       "  'typical_amount_cents': 1555},\n",
       " {'entity_name': 'IZ',\n",
       "  'income': False,\n",
       "  'period_days': 17.0,\n",
       "  'typical_amount_cents': 748},\n",
       " {'entity_name': 'JOHN',\n",
       "  'income': False,\n",
       "  'period_days': 27.0,\n",
       "  'typical_amount_cents': 57437},\n",
       " {'entity_name': 'Kapten',\n",
       "  'income': False,\n",
       "  'period_days': 3.0,\n",
       "  'typical_amount_cents': 1166},\n",
       " {'entity_name': 'LOAN',\n",
       "  'income': False,\n",
       "  'period_days': 5.0,\n",
       "  'typical_amount_cents': 786},\n",
       " {'entity_name': 'M&S',\n",
       "  'income': False,\n",
       "  'period_days': 127.0,\n",
       "  'typical_amount_cents': 640},\n",
       " {'entity_name': 'MAHIKI',\n",
       "  'income': False,\n",
       "  'period_days': 0.0,\n",
       "  'typical_amount_cents': 1246},\n",
       " {'entity_name': 'MCDONALDS',\n",
       "  'income': False,\n",
       "  'period_days': 46.0,\n",
       "  'typical_amount_cents': 738},\n",
       " {'entity_name': 'MICHAEL',\n",
       "  'income': True,\n",
       "  'period_days': 20.0,\n",
       "  'typical_amount_cents': 6950},\n",
       " {'entity_name': 'MM',\n",
       "  'income': False,\n",
       "  'period_days': 16.0,\n",
       "  'typical_amount_cents': 1020},\n",
       " {'entity_name': 'Max',\n",
       "  'income': True,\n",
       "  'period_days': 64.0,\n",
       "  'typical_amount_cents': 262194},\n",
       " {'entity_name': 'N',\n",
       "  'income': False,\n",
       "  'period_days': 15.0,\n",
       "  'typical_amount_cents': 424},\n",
       " {'entity_name': 'NANNA',\n",
       "  'income': False,\n",
       "  'period_days': 23.0,\n",
       "  'typical_amount_cents': 698},\n",
       " {'entity_name': 'NORWEGIAN',\n",
       "  'income': False,\n",
       "  'period_days': 66.0,\n",
       "  'typical_amount_cents': 14785},\n",
       " {'entity_name': 'NOTEMACHINE',\n",
       "  'income': False,\n",
       "  'period_days': 82.0,\n",
       "  'typical_amount_cents': 1750},\n",
       " {'entity_name': 'NYA',\n",
       "  'income': False,\n",
       "  'period_days': 70.0,\n",
       "  'typical_amount_cents': 223},\n",
       " {'entity_name': 'PP',\n",
       "  'income': False,\n",
       "  'period_days': 5.0,\n",
       "  'typical_amount_cents': 655},\n",
       " {'entity_name': 'PRET',\n",
       "  'income': False,\n",
       "  'period_days': 19.0,\n",
       "  'typical_amount_cents': 625},\n",
       " {'entity_name': 'PURE',\n",
       "  'income': False,\n",
       "  'period_days': 31.0,\n",
       "  'typical_amount_cents': 2849},\n",
       " {'entity_name': 'REAL',\n",
       "  'income': False,\n",
       "  'period_days': 19.0,\n",
       "  'typical_amount_cents': 363},\n",
       " {'entity_name': 'RYANAIR',\n",
       "  'income': False,\n",
       "  'period_days': 29.0,\n",
       "  'typical_amount_cents': 12301},\n",
       " {'entity_name': \"SAINSBURY'S\",\n",
       "  'income': False,\n",
       "  'period_days': 32.0,\n",
       "  'typical_amount_cents': 1920},\n",
       " {'entity_name': 'SAINSBURYS',\n",
       "  'income': False,\n",
       "  'period_days': 5.0,\n",
       "  'typical_amount_cents': 681},\n",
       " {'entity_name': 'SQ',\n",
       "  'income': False,\n",
       "  'period_days': 27.0,\n",
       "  'typical_amount_cents': 1533},\n",
       " {'entity_name': 'SUMUP',\n",
       "  'income': False,\n",
       "  'period_days': 8.0,\n",
       "  'typical_amount_cents': 600},\n",
       " {'entity_name': 'Spotify',\n",
       "  'income': False,\n",
       "  'period_days': 85.0,\n",
       "  'typical_amount_cents': 999},\n",
       " {'entity_name': 'SumUp',\n",
       "  'income': False,\n",
       "  'period_days': 18.0,\n",
       "  'typical_amount_cents': 739},\n",
       " {'entity_name': 'TESCO',\n",
       "  'income': False,\n",
       "  'period_days': 2.0,\n",
       "  'typical_amount_cents': 772},\n",
       " {'entity_name': 'TFL',\n",
       "  'income': False,\n",
       "  'period_days': 2.0,\n",
       "  'typical_amount_cents': 538},\n",
       " {'entity_name': 'THE',\n",
       "  'income': False,\n",
       "  'period_days': 23.0,\n",
       "  'typical_amount_cents': 1976},\n",
       " {'entity_name': 'TSGN',\n",
       "  'income': False,\n",
       "  'period_days': 19.0,\n",
       "  'typical_amount_cents': 2556},\n",
       " {'entity_name': 'UBER',\n",
       "  'income': False,\n",
       "  'period_days': 10.0,\n",
       "  'typical_amount_cents': 1185},\n",
       " {'entity_name': 'WASABI',\n",
       "  'income': False,\n",
       "  'period_days': 136.0,\n",
       "  'typical_amount_cents': 1140},\n",
       " {'entity_name': 'WH',\n",
       "  'income': False,\n",
       "  'period_days': 192.0,\n",
       "  'typical_amount_cents': 349},\n",
       " {'entity_name': 'WM',\n",
       "  'income': False,\n",
       "  'period_days': 24.0,\n",
       "  'typical_amount_cents': 1381},\n",
       " {'entity_name': 'WORK',\n",
       "  'income': True,\n",
       "  'period_days': 44.0,\n",
       "  'typical_amount_cents': 211195}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions = preprocessing(path_to_data)\n",
    "first_model(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_name': 'AIRBNB',\n",
       "  'income': False,\n",
       "  'period_days': 20.0,\n",
       "  'typical_amount_cents': 66687},\n",
       " {'entity_name': 'AMZN',\n",
       "  'income': False,\n",
       "  'period_days': 11.0,\n",
       "  'typical_amount_cents': 1913},\n",
       " {'entity_name': 'AMZNMktplace',\n",
       "  'income': False,\n",
       "  'period_days': 24.0,\n",
       "  'typical_amount_cents': 1361},\n",
       " {'entity_name': 'APSON',\n",
       "  'income': True,\n",
       "  'period_days': 6.0,\n",
       "  'typical_amount_cents': 73545},\n",
       " {'entity_name': 'AQUARIUM',\n",
       "  'income': False,\n",
       "  'period_days': 102.0,\n",
       "  'typical_amount_cents': 1767},\n",
       " {'entity_name': 'AROMI',\n",
       "  'income': False,\n",
       "  'period_days': 52.0,\n",
       "  'typical_amount_cents': 675},\n",
       " {'entity_name': 'Amazon',\n",
       "  'income': False,\n",
       "  'period_days': 40.0,\n",
       "  'typical_amount_cents': 1081},\n",
       " {'entity_name': 'Amazon.co.uk',\n",
       "  'income': False,\n",
       "  'period_days': 46.0,\n",
       "  'typical_amount_cents': 1497},\n",
       " {'entity_name': 'BILL',\n",
       "  'income': False,\n",
       "  'period_days': 28.0,\n",
       "  'typical_amount_cents': 25008},\n",
       " {'entity_name': 'BON',\n",
       "  'income': False,\n",
       "  'period_days': 4.0,\n",
       "  'typical_amount_cents': 680},\n",
       " {'entity_name': 'BONAPPETIT',\n",
       "  'income': False,\n",
       "  'period_days': 12.0,\n",
       "  'typical_amount_cents': 500},\n",
       " {'entity_name': 'BOOTS',\n",
       "  'income': False,\n",
       "  'period_days': 16.0,\n",
       "  'typical_amount_cents': 855},\n",
       " {'entity_name': 'BOOTS,CAMBRIDGE',\n",
       "  'income': False,\n",
       "  'period_days': 72.0,\n",
       "  'typical_amount_cents': 294},\n",
       " {'entity_name': 'CAMBRIDGE',\n",
       "  'income': False,\n",
       "  'period_days': 31.0,\n",
       "  'typical_amount_cents': 5436},\n",
       " {'entity_name': 'DELIVEROO.CO.UK',\n",
       "  'income': False,\n",
       "  'period_days': 104.0,\n",
       "  'typical_amount_cents': 1633},\n",
       " {'entity_name': 'DROPBOX',\n",
       "  'income': False,\n",
       "  'period_days': 31.0,\n",
       "  'typical_amount_cents': 919},\n",
       " {'entity_name': 'ENGOCHA',\n",
       "  'income': False,\n",
       "  'period_days': 32.0,\n",
       "  'typical_amount_cents': 969},\n",
       " {'entity_name': 'FIVE',\n",
       "  'income': False,\n",
       "  'period_days': 45.0,\n",
       "  'typical_amount_cents': 1605},\n",
       " {'entity_name': 'FOOTBALL',\n",
       "  'income': False,\n",
       "  'period_days': 36.0,\n",
       "  'typical_amount_cents': 7107},\n",
       " {'entity_name': 'GREATER',\n",
       "  'income': False,\n",
       "  'period_days': 31.0,\n",
       "  'typical_amount_cents': 2128},\n",
       " {'entity_name': 'GREEN',\n",
       "  'income': False,\n",
       "  'period_days': 58.0,\n",
       "  'typical_amount_cents': 1740},\n",
       " {'entity_name': 'Gaucho',\n",
       "  'income': False,\n",
       "  'period_days': 10.0,\n",
       "  'typical_amount_cents': 1374},\n",
       " {'entity_name': 'H',\n",
       "  'income': False,\n",
       "  'period_days': 76.0,\n",
       "  'typical_amount_cents': 3466},\n",
       " {'entity_name': 'HAWKS',\n",
       "  'income': False,\n",
       "  'period_days': 118.0,\n",
       "  'typical_amount_cents': 3167},\n",
       " {'entity_name': 'INVESTMENT',\n",
       "  'income': False,\n",
       "  'period_days': 34.0,\n",
       "  'typical_amount_cents': 28667},\n",
       " {'entity_name': 'ITUNES.COM/BILL',\n",
       "  'income': False,\n",
       "  'period_days': 44.0,\n",
       "  'typical_amount_cents': 1555},\n",
       " {'entity_name': 'IZ',\n",
       "  'income': False,\n",
       "  'period_days': 17.0,\n",
       "  'typical_amount_cents': 748},\n",
       " {'entity_name': 'JOHN',\n",
       "  'income': False,\n",
       "  'period_days': 27.0,\n",
       "  'typical_amount_cents': 57437},\n",
       " {'entity_name': 'Kapten',\n",
       "  'income': False,\n",
       "  'period_days': 3.0,\n",
       "  'typical_amount_cents': 1166},\n",
       " {'entity_name': 'LOAN',\n",
       "  'income': False,\n",
       "  'period_days': 5.0,\n",
       "  'typical_amount_cents': 786},\n",
       " {'entity_name': 'M&S',\n",
       "  'income': False,\n",
       "  'period_days': 127.0,\n",
       "  'typical_amount_cents': 640},\n",
       " {'entity_name': 'MAHIKI',\n",
       "  'income': False,\n",
       "  'period_days': 0.0,\n",
       "  'typical_amount_cents': 1246},\n",
       " {'entity_name': 'MCDONALDS',\n",
       "  'income': False,\n",
       "  'period_days': 46.0,\n",
       "  'typical_amount_cents': 738},\n",
       " {'entity_name': 'MICHAEL',\n",
       "  'income': True,\n",
       "  'period_days': 20.0,\n",
       "  'typical_amount_cents': 6950},\n",
       " {'entity_name': 'MM',\n",
       "  'income': False,\n",
       "  'period_days': 16.0,\n",
       "  'typical_amount_cents': 1020},\n",
       " {'entity_name': 'Max',\n",
       "  'income': True,\n",
       "  'period_days': 64.0,\n",
       "  'typical_amount_cents': 262194},\n",
       " {'entity_name': 'N',\n",
       "  'income': False,\n",
       "  'period_days': 15.0,\n",
       "  'typical_amount_cents': 424},\n",
       " {'entity_name': 'NANNA',\n",
       "  'income': False,\n",
       "  'period_days': 23.0,\n",
       "  'typical_amount_cents': 698},\n",
       " {'entity_name': 'NORWEGIAN',\n",
       "  'income': False,\n",
       "  'period_days': 66.0,\n",
       "  'typical_amount_cents': 14785},\n",
       " {'entity_name': 'NOTEMACHINE',\n",
       "  'income': False,\n",
       "  'period_days': 82.0,\n",
       "  'typical_amount_cents': 1750},\n",
       " {'entity_name': 'NYA',\n",
       "  'income': False,\n",
       "  'period_days': 70.0,\n",
       "  'typical_amount_cents': 223},\n",
       " {'entity_name': 'PP',\n",
       "  'income': False,\n",
       "  'period_days': 5.0,\n",
       "  'typical_amount_cents': 655},\n",
       " {'entity_name': 'PRET',\n",
       "  'income': False,\n",
       "  'period_days': 19.0,\n",
       "  'typical_amount_cents': 625},\n",
       " {'entity_name': 'PURE',\n",
       "  'income': False,\n",
       "  'period_days': 31.0,\n",
       "  'typical_amount_cents': 2849},\n",
       " {'entity_name': 'REAL',\n",
       "  'income': False,\n",
       "  'period_days': 19.0,\n",
       "  'typical_amount_cents': 363},\n",
       " {'entity_name': 'RYANAIR',\n",
       "  'income': False,\n",
       "  'period_days': 29.0,\n",
       "  'typical_amount_cents': 12301},\n",
       " {'entity_name': \"SAINSBURY'S\",\n",
       "  'income': False,\n",
       "  'period_days': 32.0,\n",
       "  'typical_amount_cents': 1920},\n",
       " {'entity_name': 'SAINSBURYS',\n",
       "  'income': False,\n",
       "  'period_days': 5.0,\n",
       "  'typical_amount_cents': 681},\n",
       " {'entity_name': 'SQ',\n",
       "  'income': False,\n",
       "  'period_days': 27.0,\n",
       "  'typical_amount_cents': 1533},\n",
       " {'entity_name': 'SUMUP',\n",
       "  'income': False,\n",
       "  'period_days': 8.0,\n",
       "  'typical_amount_cents': 600},\n",
       " {'entity_name': 'Spotify',\n",
       "  'income': False,\n",
       "  'period_days': 85.0,\n",
       "  'typical_amount_cents': 999},\n",
       " {'entity_name': 'SumUp',\n",
       "  'income': False,\n",
       "  'period_days': 18.0,\n",
       "  'typical_amount_cents': 739},\n",
       " {'entity_name': 'TESCO',\n",
       "  'income': False,\n",
       "  'period_days': 2.0,\n",
       "  'typical_amount_cents': 772},\n",
       " {'entity_name': 'TFL',\n",
       "  'income': False,\n",
       "  'period_days': 2.0,\n",
       "  'typical_amount_cents': 538},\n",
       " {'entity_name': 'THE',\n",
       "  'income': False,\n",
       "  'period_days': 23.0,\n",
       "  'typical_amount_cents': 1976},\n",
       " {'entity_name': 'TSGN',\n",
       "  'income': False,\n",
       "  'period_days': 19.0,\n",
       "  'typical_amount_cents': 2556},\n",
       " {'entity_name': 'UBER',\n",
       "  'income': False,\n",
       "  'period_days': 10.0,\n",
       "  'typical_amount_cents': 1185},\n",
       " {'entity_name': 'WASABI',\n",
       "  'income': False,\n",
       "  'period_days': 136.0,\n",
       "  'typical_amount_cents': 1140},\n",
       " {'entity_name': 'WH',\n",
       "  'income': False,\n",
       "  'period_days': 192.0,\n",
       "  'typical_amount_cents': 349},\n",
       " {'entity_name': 'WM',\n",
       "  'income': False,\n",
       "  'period_days': 24.0,\n",
       "  'typical_amount_cents': 1381},\n",
       " {'entity_name': 'WORK',\n",
       "  'income': True,\n",
       "  'period_days': 44.0,\n",
       "  'typical_amount_cents': 211195}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_model(transactions, info=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model Review<a name=\"firstmodelreview\"></a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Potential improvements."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Improvement 1. \n",
    "As we can see with the 'Spotify' example below, an old transaction on the 2018-08-06 disrupts our calculation for the 'period_days'. This recurring subscription occurs once a month ~ 30 days.\n",
    "\n",
    "A processing step to remove outlying transactions such as the transaction on the 2018-08-06 could improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old number of transactions: 6 \n",
      "        Old number of groups: 1 \n",
      "        Remaining number of transactions: 6 \n",
      "        Remaining number of groups: 1 \n",
      "\n",
      "Spotify transactions:\n",
      "   index                               user_id    made_on  \\\n",
      "0    586  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2018-08-06   \n",
      "1    889  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-06-05   \n",
      "2    909  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-07-05   \n",
      "3   1016  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-08-04   \n",
      "4   1097  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-09-04   \n",
      "5   1208  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-10-05   \n",
      "\n",
      "               original_description  amount_cents  income  \\\n",
      "0  Spotify P06EAA1A46 ON 05 AUG BCC           999   False   \n",
      "1  Spotify P08VAZ2H51 ON 04 JUN CLP           999   False   \n",
      "2  Spotify P07YAB3H83 ON 04 JUL CLP           999   False   \n",
      "3          Spotify UK ON 03 AUG BCC           999   False   \n",
      "4  Spotify P0D97DK26E ON 03 SEP BCC           999   False   \n",
      "5  Spotify P0D97JL34E ON 04 OCT BCC           999   False   \n",
      "\n",
      "  original_description_first_word  time_diff_forward  \n",
      "0                         Spotify              303.0  \n",
      "1                         Spotify               30.0  \n",
      "2                         Spotify               30.0  \n",
      "3                         Spotify               31.0  \n",
      "4                         Spotify               31.0  \n",
      "5                         Spotify                NaN  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_name': 'Spotify',\n",
       "  'income': False,\n",
       "  'period_days': 85.0,\n",
       "  'typical_amount_cents': 999}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_model(transactions, example_name = 'Spotify')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Improvement 2.\n",
    "Since we haven't filtered any of the transactions in a group which could skew our mean for the typical amount in cents, for example, a monthly phone subscription of £30 but with a first time payment of £200. I could remove those far from the mean for 'amount_cents' to exclude these one of payments. Or remove"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Improvement 3.\n",
    "After the improvements in 1 and 2, I could end labelling a group of recurring payments which have less than three occurances in the bank transactions - post removing outliers. I could perform another remove_groups_too_small function or similar to handle this."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Improvement 4. \n",
    "The desired output for the \"entity_name\" could be improved and categorized. For the 'Spotify' example it could provide more information, such that its a monthly occuring transaction, or/and that its a subscription."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Improvement 5.\n",
    "Many of the outputs for \"entity_name\" are all capital letters, for example, 'BILL'. This is grammatically i incorrect and we would like our output to be gramatically correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model<a name=\"finalmodel\"></a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Improvement 1 solution.\n",
    "To take care of outliers within a group I shall create an upper_lim and lower_lim, if the 'time_diff_forward' is not between these limits remove the entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_time_difference(group, factor, info=True):\n",
    "\n",
    "    upper_lim = group['time_diff_forward'].mean(skipna=True) + group['time_diff_forward'].std(skipna=True) * factor\n",
    "    lower_lim = group['time_diff_forward'].mean(skipna=True) - group['time_diff_forward'].std(skipna=True) * factor\n",
    "\n",
    "    # I don't want to remove the most recent transaction where 'time_diff_forward' = Nan. So I will save it in last_row, and remove it temporarily from dataframe.\n",
    "    last_row = group.tail(1)\n",
    "    group.drop(group.tail(1).index,inplace=True)\n",
    "\n",
    "    # remove outliers.\n",
    "    new_group = group[(lower_lim < group['time_diff_forward']) & (group['time_diff_forward'] < upper_lim) & (group['time_diff_forward'] != np.nan) ]\n",
    "    \n",
    "    if info:\n",
    "        print(\"For {} removed {} entries. Now {} entries remaining.\".format(group['original_description_first_word'].iloc[0], group['user_id'].count() - new_group['user_id'].count(), new_group['user_id'].count()+1))\n",
    "\n",
    "    group = new_group\n",
    "\n",
    "    # Append most recent transcation where 'time_diff_forward' = Nan.\n",
    "    group = group.append(last_row, ignore_index=True)\n",
    "    \n",
    "    return group"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Improvement 2 solution.\n",
    "Remove transactions in a group which are far from the mean of the groups transactions, such as one of payments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_amount_cents(group, factor, info=True):\n",
    "\n",
    "    upper_lim = group['amount_cents'].mean() + group['amount_cents'].std() * factor\n",
    "    lower_lim = group['amount_cents'].mean() - group['amount_cents'].std() * factor\n",
    "\n",
    "    # remove outliers.\n",
    "    new_group = group[(lower_lim < group['amount_cents']) & (group['amount_cents'] < upper_lim)]\n",
    "    \n",
    "    if info:\n",
    "        print(\"For {} removed {} entries. Now {} entries remaining.\".format(group['original_description_first_word'].iloc[0], group['user_id'].count() - new_group['user_id'].count(), new_group['user_id'].count()))\n",
    "\n",
    "    group = new_group\n",
    "    \n",
    "    return group"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Improvement 3 solution.\n",
    "In final_model, I add a condition to only add a group to recurring output list if group size is not less than lower_minimum (default value of 3). As removing outliers may have left groups too small to be classified as recurring."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Improvement 4 solution.\n",
    "Recurring payments tend to be categorized as annually, monthly, bi-weekly, weekly or daily. So I shall categorize them based on each groups 'time_diff_forward' mean, whether that mean falls within realistic bounds. The boundaries I've used have been down to my discretion on what I think is reasonable. For example, annually is 364 days +- 4 days. This is an assumption which could be improved if I had more time by looking at other data on the distribution of time differences forward with known category targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_recurring_and_label(group):\n",
    "    if (364 - 4 < group['time_diff_forward'].mean(skipna=True) < 364 + 4):\n",
    "        label = 'Annually'\n",
    "        return True, label\n",
    "    if (364/12 - 3  < group['time_diff_forward'].mean(skipna=True) < 364/12 + 3):\n",
    "        label = 'Monthly'\n",
    "        return True, label\n",
    "    if (14 - 2 < group['time_diff_forward'].mean(skipna=True) < 14 + 2):\n",
    "        label = 'Bi-weekly'\n",
    "    if (7 - 1 < group['time_diff_forward'].mean(skipna=True) < 7 + 1):\n",
    "        label = 'Weekly'\n",
    "        return True, label\n",
    "    if (1 + 0.25 < group['time_diff_forward'].mean(skipna=True) < 1 + 0.25):\n",
    "        label = 'Daily'\n",
    "        return True, label\n",
    "    else:\n",
    "        return False, \"Not recurring\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Improvement 5 solution.\n",
    "I shall call the capitalize() in-built python function on the 'original_description_first_word' before appending the dictionary to the output list.\n",
    "A potential problem with this approach is names which are genuinely all capital letters such as a subscription to TIDAL music (not present in this dataset but to demonstrate my thinking)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Putting it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model(transactions, info=True, example_name=None, lower_minimum=3):\n",
    "    \"\"\" Second model for get_recurring_transactions() function.\n",
    "    Returns recurring, a list of dictionaries where each dictionary is of form:\n",
    "    \n",
    "    {\"entity_name\": A common description of the regularly occurring transactions,\n",
    "    \"income\": A boolean whether the transaction is debit (True) or credit (False),\n",
    "    \"period_days\": The expected interval in days between consecutive transactions for that entity, \n",
    "    \"typical_amount_cents\": The typical amount in cents}\n",
    "    \n",
    "    input info: whether or not to display processing information,\n",
    "    input example_name: name of example to make it easy to select a specific group example for analysis later,\n",
    "    input lower_minimum: lower limit on the size of a group to not be removed, default = 3.\n",
    "    \"\"\"  \n",
    "    transactions = parse_original_description(transactions)\n",
    "    \n",
    "    # Code to make it easy to investigate specific example e.g 'Spotify'\n",
    "    if example_name is not None:\n",
    "        transactions = transactions[transactions['original_description_first_word'] == example_name]\n",
    "                                                                                           \n",
    "    transactions = remove_groups_too_small(transactions, info)\n",
    "    \n",
    "    recurring = []\n",
    "    \n",
    "    for name, group in transactions.groupby('original_description_first_word'):\n",
    "        group = group_create_time_difference_column(group)\n",
    "        \n",
    "        if info:\n",
    "            print(\"Removing outliers for {}\".format(name))\n",
    "            \n",
    "        group = remove_outliers_time_difference(group, factor = 1, info=info)\n",
    "        group = remove_outliers_amount_cents(group, factor = 1, info = info)\n",
    "        \n",
    "        if name == example_name:\n",
    "            print(\"{} transactions:\\n{}\".format(example_name, group))\n",
    "        \n",
    "        recurring_boolean, label = check_if_recurring_and_label(group)\n",
    "        \n",
    "        if recurring_boolean & (group['user_id'].count() >= lower_minimum):\n",
    "            recurring.append({\"entity_name\": label+\" \"+name.capitalize(), \"income\": group['income'].all() ,\\\n",
    "                          \"period_days\": round(group['time_diff_forward'].mean(skipna=True)),\\\n",
    "                          \"typical_amount_cents\": int(round(group['amount_cents'].mean()))})\n",
    "    return recurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old number of transactions: 1210 \n",
      "        Old number of groups: 291 \n",
      "        Remaining number of transactions: 928 \n",
      "        Remaining number of groups: 61 \n",
      "\n",
      "Removing outliers for AIRBNB\n",
      "For AIRBNB removed 0 entries. Now 3 entries remaining.\n",
      "For AIRBNB removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for AMZN\n",
      "For AMZN removed 0 entries. Now 3 entries remaining.\n",
      "For AMZN removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for AMZNMktplace\n",
      "For AMZNMktplace removed 3 entries. Now 5 entries remaining.\n",
      "For AMZNMktplace removed 1 entries. Now 4 entries remaining.\n",
      "Removing outliers for APSON\n",
      "For APSON removed 0 entries. Now 3 entries remaining.\n",
      "For APSON removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for AQUARIUM\n",
      "For AQUARIUM removed 0 entries. Now 3 entries remaining.\n",
      "For AQUARIUM removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for AROMI\n",
      "For AROMI removed 1 entries. Now 7 entries remaining.\n",
      "For AROMI removed 2 entries. Now 5 entries remaining.\n",
      "Removing outliers for Amazon\n",
      "For Amazon removed 3 entries. Now 8 entries remaining.\n",
      "For Amazon removed 1 entries. Now 7 entries remaining.\n",
      "Removing outliers for Amazon.co.uk\n",
      "For Amazon.co.uk removed 1 entries. Now 4 entries remaining.\n",
      "For Amazon.co.uk removed 1 entries. Now 3 entries remaining.\n",
      "Removing outliers for BILL\n",
      "For BILL removed 1 entries. Now 3 entries remaining.\n",
      "For BILL removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for BON\n",
      "For BON removed 1 entries. Now 4 entries remaining.\n",
      "For BON removed 0 entries. Now 4 entries remaining.\n",
      "Removing outliers for BONAPPETIT\n",
      "For BONAPPETIT removed 1 entries. Now 3 entries remaining.\n",
      "For BONAPPETIT removed 3 entries. Now 0 entries remaining.\n",
      "Removing outliers for BOOTS\n",
      "For BOOTS removed 2 entries. Now 7 entries remaining.\n",
      "For BOOTS removed 2 entries. Now 5 entries remaining.\n",
      "Removing outliers for BOOTS,CAMBRIDGE\n",
      "For BOOTS,CAMBRIDGE removed 1 entries. Now 5 entries remaining.\n",
      "For BOOTS,CAMBRIDGE removed 1 entries. Now 4 entries remaining.\n",
      "Removing outliers for CAMBRIDGE\n",
      "For CAMBRIDGE removed 2 entries. Now 7 entries remaining.\n",
      "For CAMBRIDGE removed 3 entries. Now 4 entries remaining.\n",
      "Removing outliers for DELIVEROO.CO.UK\n",
      "For DELIVEROO.CO.UK removed 1 entries. Now 3 entries remaining.\n",
      "For DELIVEROO.CO.UK removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for DROPBOX\n",
      "For DROPBOX removed 1 entries. Now 4 entries remaining.\n",
      "For DROPBOX removed 0 entries. Now 4 entries remaining.\n",
      "Removing outliers for ENGOCHA\n",
      "For ENGOCHA removed 1 entries. Now 7 entries remaining.\n",
      "For ENGOCHA removed 1 entries. Now 6 entries remaining.\n",
      "Removing outliers for FIVE\n",
      "For FIVE removed 1 entries. Now 4 entries remaining.\n",
      "For FIVE removed 1 entries. Now 3 entries remaining.\n",
      "Removing outliers for FOOTBALL\n",
      "For FOOTBALL removed 2 entries. Now 5 entries remaining.\n",
      "For FOOTBALL removed 1 entries. Now 4 entries remaining.\n",
      "Removing outliers for GREATER\n",
      "For GREATER removed 2 entries. Now 12 entries remaining.\n",
      "For GREATER removed 1 entries. Now 11 entries remaining.\n",
      "Removing outliers for GREEN\n",
      "For GREEN removed 0 entries. Now 3 entries remaining.\n",
      "For GREEN removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for Gaucho\n",
      "For Gaucho removed 0 entries. Now 3 entries remaining.\n",
      "For Gaucho removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for H\n",
      "For H removed 0 entries. Now 3 entries remaining.\n",
      "For H removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for HAWKS\n",
      "For HAWKS removed 0 entries. Now 3 entries remaining.\n",
      "For HAWKS removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for INVESTMENT\n",
      "For INVESTMENT removed 0 entries. Now 3 entries remaining.\n",
      "For INVESTMENT removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for ITUNES.COM/BILL\n",
      "For ITUNES.COM/BILL removed 4 entries. Now 4 entries remaining.\n",
      "For ITUNES.COM/BILL removed 1 entries. Now 3 entries remaining.\n",
      "Removing outliers for IZ\n",
      "For IZ removed 1 entries. Now 25 entries remaining.\n",
      "For IZ removed 8 entries. Now 17 entries remaining.\n",
      "Removing outliers for JOHN\n",
      "For JOHN removed 1 entries. Now 14 entries remaining.\n",
      "For JOHN removed 3 entries. Now 11 entries remaining.\n",
      "Removing outliers for Kapten\n",
      "For Kapten removed 0 entries. Now 3 entries remaining.\n",
      "For Kapten removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for LOAN\n",
      "For LOAN removed 3 entries. Now 4 entries remaining.\n",
      "For LOAN removed 1 entries. Now 3 entries remaining.\n",
      "Removing outliers for M&S\n",
      "For M&S removed 1 entries. Now 3 entries remaining.\n",
      "For M&S removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for MAHIKI\n",
      "For MAHIKI removed 2 entries. Now 1 entries remaining.\n",
      "For MAHIKI removed 1 entries. Now 0 entries remaining.\n",
      "Removing outliers for MCDONALDS\n",
      "For MCDONALDS removed 1 entries. Now 8 entries remaining.\n",
      "For MCDONALDS removed 1 entries. Now 7 entries remaining.\n",
      "Removing outliers for MICHAEL\n",
      "For MICHAEL removed 0 entries. Now 3 entries remaining.\n",
      "For MICHAEL removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for MM\n",
      "For MM removed 1 entries. Now 14 entries remaining.\n",
      "For MM removed 1 entries. Now 13 entries remaining.\n",
      "Removing outliers for Max\n",
      "For Max removed 1 entries. Now 3 entries remaining.\n",
      "For Max removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for N\n",
      "For N removed 5 entries. Now 8 entries remaining.\n",
      "For N removed 2 entries. Now 6 entries remaining.\n",
      "Removing outliers for NANNA\n",
      "For NANNA removed 1 entries. Now 5 entries remaining.\n",
      "For NANNA removed 2 entries. Now 3 entries remaining.\n",
      "Removing outliers for NORWEGIAN\n",
      "For NORWEGIAN removed 1 entries. Now 3 entries remaining.\n",
      "For NORWEGIAN removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for NOTEMACHINE\n",
      "For NOTEMACHINE removed 1 entries. Now 3 entries remaining.\n",
      "For NOTEMACHINE removed 3 entries. Now 0 entries remaining.\n",
      "Removing outliers for NYA\n",
      "For NYA removed 1 entries. Now 5 entries remaining.\n",
      "For NYA removed 1 entries. Now 4 entries remaining.\n",
      "Removing outliers for PP\n",
      "For PP removed 1 entries. Now 59 entries remaining.\n",
      "For PP removed 17 entries. Now 42 entries remaining.\n",
      "Removing outliers for PRET\n",
      "For PRET removed 3 entries. Now 20 entries remaining.\n",
      "For PRET removed 6 entries. Now 14 entries remaining.\n",
      "Removing outliers for PURE\n",
      "For PURE removed 3 entries. Now 4 entries remaining.\n",
      "For PURE removed 4 entries. Now 0 entries remaining.\n",
      "Removing outliers for REAL\n",
      "For REAL removed 1 entries. Now 5 entries remaining.\n",
      "For REAL removed 3 entries. Now 2 entries remaining.\n",
      "Removing outliers for RYANAIR\n",
      "For RYANAIR removed 2 entries. Now 3 entries remaining.\n",
      "For RYANAIR removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for SAINSBURY'S\n",
      "For SAINSBURY'S removed 2 entries. Now 6 entries remaining.\n",
      "For SAINSBURY'S removed 1 entries. Now 5 entries remaining.\n",
      "Removing outliers for SAINSBURYS\n",
      "For SAINSBURYS removed 2 entries. Now 82 entries remaining.\n",
      "For SAINSBURYS removed 6 entries. Now 76 entries remaining.\n",
      "Removing outliers for SQ\n",
      "For SQ removed 0 entries. Now 3 entries remaining.\n",
      "For SQ removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for SUMUP\n",
      "For SUMUP removed 1 entries. Now 5 entries remaining.\n",
      "For SUMUP removed 2 entries. Now 3 entries remaining.\n",
      "Removing outliers for Spotify\n",
      "For Spotify removed 1 entries. Now 5 entries remaining.\n",
      "For Spotify removed 5 entries. Now 0 entries remaining.\n",
      "Removing outliers for SumUp\n",
      "For SumUp removed 1 entries. Now 15 entries remaining.\n",
      "For SumUp removed 4 entries. Now 11 entries remaining.\n",
      "Removing outliers for TESCO\n",
      "For TESCO removed 5 entries. Now 135 entries remaining.\n",
      "For TESCO removed 7 entries. Now 128 entries remaining.\n",
      "Removing outliers for TFL\n",
      "For TFL removed 2 entries. Now 208 entries remaining.\n",
      "For TFL removed 32 entries. Now 176 entries remaining.\n",
      "Removing outliers for THE\n",
      "For THE removed 1 entries. Now 18 entries remaining.\n",
      "For THE removed 7 entries. Now 11 entries remaining.\n",
      "Removing outliers for TSGN\n",
      "For TSGN removed 3 entries. Now 6 entries remaining.\n",
      "For TSGN removed 0 entries. Now 6 entries remaining.\n",
      "Removing outliers for UBER\n",
      "For UBER removed 1 entries. Now 39 entries remaining.\n",
      "For UBER removed 4 entries. Now 35 entries remaining.\n",
      "Removing outliers for WASABI\n",
      "For WASABI removed 0 entries. Now 3 entries remaining.\n",
      "For WASABI removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for WH\n",
      "For WH removed 0 entries. Now 3 entries remaining.\n",
      "For WH removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for WM\n",
      "For WM removed 0 entries. Now 3 entries remaining.\n",
      "For WM removed 1 entries. Now 2 entries remaining.\n",
      "Removing outliers for WORK\n",
      "For WORK removed 1 entries. Now 7 entries remaining.\n",
      "For WORK removed 2 entries. Now 5 entries remaining.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_name': 'Monthly Amznmktplace',\n",
       "  'income': False,\n",
       "  'period_days': 30.0,\n",
       "  'typical_amount_cents': 724},\n",
       " {'entity_name': 'Monthly Amazon',\n",
       "  'income': False,\n",
       "  'period_days': 30.0,\n",
       "  'typical_amount_cents': 799},\n",
       " {'entity_name': 'Monthly Dropbox',\n",
       "  'income': False,\n",
       "  'period_days': 30.0,\n",
       "  'typical_amount_cents': 899},\n",
       " {'entity_name': \"Monthly Sainsbury's\",\n",
       "  'income': False,\n",
       "  'period_days': 31.0,\n",
       "  'typical_amount_cents': 1251},\n",
       " {'entity_name': 'Monthly Work',\n",
       "  'income': True,\n",
       "  'period_days': 30.0,\n",
       "  'typical_amount_cents': 178243}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions = preprocessing(path_to_data)\n",
    "final_model(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_name': 'Monthly Amznmktplace',\n",
       "  'income': False,\n",
       "  'period_days': 30.0,\n",
       "  'typical_amount_cents': 724},\n",
       " {'entity_name': 'Monthly Amazon',\n",
       "  'income': False,\n",
       "  'period_days': 30.0,\n",
       "  'typical_amount_cents': 799},\n",
       " {'entity_name': 'Monthly Dropbox',\n",
       "  'income': False,\n",
       "  'period_days': 30.0,\n",
       "  'typical_amount_cents': 899},\n",
       " {'entity_name': \"Monthly Sainsbury's\",\n",
       "  'income': False,\n",
       "  'period_days': 31.0,\n",
       "  'typical_amount_cents': 1251},\n",
       " {'entity_name': 'Monthly Work',\n",
       "  'income': True,\n",
       "  'period_days': 30.0,\n",
       "  'typical_amount_cents': 178243}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model(transactions, info=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Review<a name=\"finalmodelreview\"></a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Improvement 1.\n",
    "Potential problem with assumption that recurring transcations have same first word.\n",
    "Here 'N' is first word using method, there could be two different entities making diffierent transcations with 'N' as first name. For example, \"N Y Stock Exhange\" and \"N K CONVENIENCE\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>made_on</th>\n",
       "      <th>original_description</th>\n",
       "      <th>amount_cents</th>\n",
       "      <th>income</th>\n",
       "      <th>original_description_first_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-03-25</td>\n",
       "      <td>N K CONVENIENCE ST ON 23 MAR CLP</td>\n",
       "      <td>338</td>\n",
       "      <td>False</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>N K CONVENIENCE ST ON 31 MAR CLP</td>\n",
       "      <td>747</td>\n",
       "      <td>False</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-05-02</td>\n",
       "      <td>N K CONVENIENCE ST ON 01 MAY CLP</td>\n",
       "      <td>347</td>\n",
       "      <td>False</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-06-06</td>\n",
       "      <td>N K CONVENIENCE ST ON 05 JUN CLP</td>\n",
       "      <td>249</td>\n",
       "      <td>False</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-06-24</td>\n",
       "      <td>N K CONVENIENCE ST ON 23 JUN CLP</td>\n",
       "      <td>473</td>\n",
       "      <td>False</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  user_id    made_on  \\\n",
       "387  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-03-25   \n",
       "368  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-04-01   \n",
       "858  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-05-02   \n",
       "185  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-06-06   \n",
       "123  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-06-24   \n",
       "\n",
       "                 original_description  amount_cents  income  \\\n",
       "387  N K CONVENIENCE ST ON 23 MAR CLP           338   False   \n",
       "368  N K CONVENIENCE ST ON 31 MAR CLP           747   False   \n",
       "858  N K CONVENIENCE ST ON 01 MAY CLP           347   False   \n",
       "185  N K CONVENIENCE ST ON 05 JUN CLP           249   False   \n",
       "123  N K CONVENIENCE ST ON 23 JUN CLP           473   False   \n",
       "\n",
       "    original_description_first_word  \n",
       "387                               N  \n",
       "368                               N  \n",
       "858                               N  \n",
       "185                               N  \n",
       "123                               N  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions = preprocessing(path_to_data)\n",
    "transactions = parse_original_description(transactions)\n",
    "transactions[transactions['original_description_first_word'] == 'N'].head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Potential problem not a problem in this data however could be in unseen data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Improvement 2.\n",
    "Another problem with our assumption that recurring transactions have the same first word can be seen below, where two different bills are grouped together, [\"BILL KENNEDY SPLITWISE\", \"BILL A+SESSA M SWEDEN\"]. This could be problematic in larger data sets if there was two different recurring payments both with first word \"BILL\".\n",
    "A potential solution could begin to include the second and third words in \"original_description\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>made_on</th>\n",
       "      <th>original_description</th>\n",
       "      <th>amount_cents</th>\n",
       "      <th>income</th>\n",
       "      <th>original_description_first_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-03-22</td>\n",
       "      <td>BILL KENNEDY SPLITWISE FT</td>\n",
       "      <td>25255</td>\n",
       "      <td>False</td>\n",
       "      <td>BILL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-05-05</td>\n",
       "      <td>BILL KENNEDY SPLITWISE FT</td>\n",
       "      <td>26165</td>\n",
       "      <td>False</td>\n",
       "      <td>BILL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-06-06</td>\n",
       "      <td>BILL KENNEDY SPLITWISE FT</td>\n",
       "      <td>33714</td>\n",
       "      <td>False</td>\n",
       "      <td>BILL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>BILL A+SESSA M SWEDEN - ALBERT BBP</td>\n",
       "      <td>14900</td>\n",
       "      <td>True</td>\n",
       "      <td>BILL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  user_id    made_on  \\\n",
       "398  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-03-22   \n",
       "282  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-05-05   \n",
       "184  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-06-06   \n",
       "161  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-06-13   \n",
       "\n",
       "                   original_description  amount_cents  income  \\\n",
       "398           BILL KENNEDY SPLITWISE FT         25255   False   \n",
       "282           BILL KENNEDY SPLITWISE FT         26165   False   \n",
       "184           BILL KENNEDY SPLITWISE FT         33714   False   \n",
       "161  BILL A+SESSA M SWEDEN - ALBERT BBP         14900    True   \n",
       "\n",
       "    original_description_first_word  \n",
       "398                            BILL  \n",
       "282                            BILL  \n",
       "184                            BILL  \n",
       "161                            BILL  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions = preprocessing(path_to_data)\n",
    "transactions = parse_original_description(transactions)\n",
    "\n",
    "transactions[transactions['original_description_first_word'] == 'BILL']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A more severe problem can be seen below, our model failed to capture monthly rent payments to John Smith.\n",
    "Grouping \"JOHN SMITH\" and \"JOHN LEWIS\" together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>made_on</th>\n",
       "      <th>original_description</th>\n",
       "      <th>amount_cents</th>\n",
       "      <th>income</th>\n",
       "      <th>original_description_first_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2018-08-10</td>\n",
       "      <td>JOHN SMITH HELLO BGC</td>\n",
       "      <td>3000</td>\n",
       "      <td>True</td>\n",
       "      <td>JOHN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2018-11-29</td>\n",
       "      <td>JOHN SMITH MINIGOLF BBP</td>\n",
       "      <td>1400</td>\n",
       "      <td>False</td>\n",
       "      <td>JOHN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>JOHN SMITH PART DEPOSIT BBP</td>\n",
       "      <td>100000</td>\n",
       "      <td>False</td>\n",
       "      <td>JOHN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>JOHN SMITH PART DEPOSIT BBP</td>\n",
       "      <td>224858</td>\n",
       "      <td>False</td>\n",
       "      <td>JOHN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>JOHN SMITH DAVID AIRBNB BBP</td>\n",
       "      <td>6700</td>\n",
       "      <td>False</td>\n",
       "      <td>JOHN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-02-27</td>\n",
       "      <td>JOHN SMITH RENT ETC STO</td>\n",
       "      <td>70236</td>\n",
       "      <td>False</td>\n",
       "      <td>JOHN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-03-27</td>\n",
       "      <td>JOHN SMITH RENT ETC STO</td>\n",
       "      <td>70236</td>\n",
       "      <td>False</td>\n",
       "      <td>JOHN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-04-24</td>\n",
       "      <td>JOHN SMITH RENT ETC STO</td>\n",
       "      <td>70236</td>\n",
       "      <td>False</td>\n",
       "      <td>JOHN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-04-28</td>\n",
       "      <td>JOHN LEWIS ON 27 APR BCC</td>\n",
       "      <td>9430</td>\n",
       "      <td>False</td>\n",
       "      <td>JOHN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-04-29</td>\n",
       "      <td>JOHN LEWIS ON 28 APR BCC</td>\n",
       "      <td>23005</td>\n",
       "      <td>False</td>\n",
       "      <td>JOHN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-05-22</td>\n",
       "      <td>JOHN SMITH RENT ETC STO</td>\n",
       "      <td>70236</td>\n",
       "      <td>False</td>\n",
       "      <td>JOHN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>JOHN SMITH RENT ETC STO</td>\n",
       "      <td>70236</td>\n",
       "      <td>False</td>\n",
       "      <td>JOHN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-07-19</td>\n",
       "      <td>JOHN LEWIS ON 18 JUL CLP</td>\n",
       "      <td>1505</td>\n",
       "      <td>False</td>\n",
       "      <td>JOHN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-07-29</td>\n",
       "      <td>JOHN SMITH RENT ETC STO</td>\n",
       "      <td>70236</td>\n",
       "      <td>False</td>\n",
       "      <td>JOHN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-08-28</td>\n",
       "      <td>JOHN SMITH RENT ETC STO</td>\n",
       "      <td>70236</td>\n",
       "      <td>False</td>\n",
       "      <td>JOHN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  user_id    made_on  \\\n",
       "582  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2018-08-10   \n",
       "522  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2018-11-29   \n",
       "517  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-01-21   \n",
       "497  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-02-04   \n",
       "496  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-02-04   \n",
       "454  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-02-27   \n",
       "384  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-03-27   \n",
       "318  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-04-24   \n",
       "305  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-04-28   \n",
       "299  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-04-29   \n",
       "234  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-05-22   \n",
       "111  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-06-28   \n",
       "35   9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-07-19   \n",
       "686  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-07-29   \n",
       "986  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-08-28   \n",
       "\n",
       "            original_description  amount_cents  income  \\\n",
       "582         JOHN SMITH HELLO BGC          3000    True   \n",
       "522      JOHN SMITH MINIGOLF BBP          1400   False   \n",
       "517  JOHN SMITH PART DEPOSIT BBP        100000   False   \n",
       "497  JOHN SMITH PART DEPOSIT BBP        224858   False   \n",
       "496  JOHN SMITH DAVID AIRBNB BBP          6700   False   \n",
       "454      JOHN SMITH RENT ETC STO         70236   False   \n",
       "384      JOHN SMITH RENT ETC STO         70236   False   \n",
       "318      JOHN SMITH RENT ETC STO         70236   False   \n",
       "305     JOHN LEWIS ON 27 APR BCC          9430   False   \n",
       "299     JOHN LEWIS ON 28 APR BCC         23005   False   \n",
       "234      JOHN SMITH RENT ETC STO         70236   False   \n",
       "111      JOHN SMITH RENT ETC STO         70236   False   \n",
       "35      JOHN LEWIS ON 18 JUL CLP          1505   False   \n",
       "686      JOHN SMITH RENT ETC STO         70236   False   \n",
       "986      JOHN SMITH RENT ETC STO         70236   False   \n",
       "\n",
       "    original_description_first_word  \n",
       "582                            JOHN  \n",
       "522                            JOHN  \n",
       "517                            JOHN  \n",
       "497                            JOHN  \n",
       "496                            JOHN  \n",
       "454                            JOHN  \n",
       "384                            JOHN  \n",
       "318                            JOHN  \n",
       "305                            JOHN  \n",
       "299                            JOHN  \n",
       "234                            JOHN  \n",
       "111                            JOHN  \n",
       "35                             JOHN  \n",
       "686                            JOHN  \n",
       "986                            JOHN  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions = preprocessing(path_to_data)\n",
    "transactions = parse_original_description(transactions)\n",
    "\n",
    "transactions[transactions['original_description_first_word'] == 'JOHN']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If our assumption to group transactions by having same first word was updated to include the second and third words, we would have to be careful to still group transactions such as the \"Spotify\" transactons below.\n",
    "\n",
    "A suggestion could be to investigate the occurances of given strings, such as \"JOHN\" occuring often, and \"JOHN SMITH\" occuring often. Where similarly \"Spotify\" occurs often, differently \"Spotify P06EAA1A46\" would only occur once, making it an unlikely candidate name for a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>made_on</th>\n",
       "      <th>original_description</th>\n",
       "      <th>amount_cents</th>\n",
       "      <th>income</th>\n",
       "      <th>original_description_first_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2018-08-06</td>\n",
       "      <td>Spotify P06EAA1A46 ON 05 AUG BCC</td>\n",
       "      <td>999</td>\n",
       "      <td>False</td>\n",
       "      <td>Spotify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-06-05</td>\n",
       "      <td>Spotify P08VAZ2H51 ON 04 JUN CLP</td>\n",
       "      <td>999</td>\n",
       "      <td>False</td>\n",
       "      <td>Spotify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>Spotify P07YAB3H83 ON 04 JUL CLP</td>\n",
       "      <td>999</td>\n",
       "      <td>False</td>\n",
       "      <td>Spotify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-08-04</td>\n",
       "      <td>Spotify UK ON 03 AUG BCC</td>\n",
       "      <td>999</td>\n",
       "      <td>False</td>\n",
       "      <td>Spotify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-09-04</td>\n",
       "      <td>Spotify P0D97DK26E ON 03 SEP BCC</td>\n",
       "      <td>999</td>\n",
       "      <td>False</td>\n",
       "      <td>Spotify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>9d556712-ff2e-2f9c-73a9-5hf3cce9e389</td>\n",
       "      <td>2019-10-05</td>\n",
       "      <td>Spotify P0D97JL34E ON 04 OCT BCC</td>\n",
       "      <td>999</td>\n",
       "      <td>False</td>\n",
       "      <td>Spotify</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   user_id    made_on  \\\n",
       "586   9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2018-08-06   \n",
       "889   9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-06-05   \n",
       "909   9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-07-05   \n",
       "1016  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-08-04   \n",
       "1097  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-09-04   \n",
       "1208  9d556712-ff2e-2f9c-73a9-5hf3cce9e389 2019-10-05   \n",
       "\n",
       "                  original_description  amount_cents  income  \\\n",
       "586   Spotify P06EAA1A46 ON 05 AUG BCC           999   False   \n",
       "889   Spotify P08VAZ2H51 ON 04 JUN CLP           999   False   \n",
       "909   Spotify P07YAB3H83 ON 04 JUL CLP           999   False   \n",
       "1016          Spotify UK ON 03 AUG BCC           999   False   \n",
       "1097  Spotify P0D97DK26E ON 03 SEP BCC           999   False   \n",
       "1208  Spotify P0D97JL34E ON 04 OCT BCC           999   False   \n",
       "\n",
       "     original_description_first_word  \n",
       "586                          Spotify  \n",
       "889                          Spotify  \n",
       "909                          Spotify  \n",
       "1016                         Spotify  \n",
       "1097                         Spotify  \n",
       "1208                         Spotify  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions = preprocessing(path_to_data)\n",
    "transactions = parse_original_description(transactions)\n",
    "\n",
    "transactions[transactions['original_description_first_word'] == 'Spotify']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion & Next Steps <a name=\"nextsteps\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used a rule based model here because its easily interpretable, explainable and our assumptions known. The problem with this model is that you have to write rules and assumptions to explain how a human would classify and process the data, which may not be explicitly writable in rules. With this approach the programmer has to think very hard about all the different assumptions and rules they're making. I found lots of problems or special cases to address, on further iterations this could improve a lot, but maybe theres a better way.\n",
    "\n",
    "The next steps could be to use more complex approaches, which may yield better results. Using a machine learning based model, potentially using natural language processing model to classify whether a subscription, or salary or a friday McDonalds - could work better. To do so would mean creating target features for a training data set which could be done if given more time, and would be a potential next step in builiding a better model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
